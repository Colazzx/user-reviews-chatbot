{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the Libraries Needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS, DistanceStrategy\n",
    "from langchain.chains import RetrievalQA\n",
    "from collections import Counter\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_openai import ChatOpenAI\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime, timedelta\n",
    "from secret_key import openai_key\n",
    "import re\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up the LLM Used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colaz\\AppData\\Local\\Temp\\ipykernel_9092\\2534856997.py:5: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.4)\n"
     ]
    }
   ],
   "source": [
    "# API Key for OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = openai_key\n",
    "\n",
    "# Set up the OpenAI chat model\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.4)\n",
    "# llm = ChatOpenAI(temperature=0.3, model=\"gpt-4o\", max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CSV Loader\n",
    "\n",
    "# loader = CSVLoader(file_path='cleaned_spotify_reviews.csv', source_column=\"review_text\", encoding='MacRoman')\n",
    "# docs = loader.load()\n",
    "\n",
    "# # # Create a list of Document instances from the loaded documents\n",
    "# # documents = [\n",
    "# #     Document(\n",
    "# #         page_content=doc['review_text'],  # Accessing the text content of the Document\n",
    "# #         metadata={\n",
    "# #             \"review_rating\": doc['review_rating'],\n",
    "# #             \"review_likes\": doc['review_likes'],\n",
    "# #             \"author_app_version\": doc['author_app_version']\n",
    "# #         }\n",
    "# #     )\n",
    "# #     for doc in docs  # Iterate over the list of loaded Document objects\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# # Embeddings and Vector Database\n",
    "# # instructor_embeddings = HuggingFaceInstructEmbeddings()\n",
    "# # instructor_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# # vectordb = FAISS.from_documents(documents=docs, embedding=instructor_embeddings)\n",
    "\n",
    "# instructor_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Initialize the text splitter with desired chunk size and overlap\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#                     chunk_size=512, \n",
    "#                     chunk_overlap=24\n",
    "# )\n",
    "\n",
    "# df = pd.read_csv('cleaned_spotify_reviews.csv', encoding='MacRoman').dropna()\n",
    "\n",
    "# # Initialize an empty list to hold documents\n",
    "# all_documents = []\n",
    "\n",
    "# # Use tqdm for progress bar\n",
    "# for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "#     # Use the text splitter to split the review_text\n",
    "#     chunks = text_splitter.split_text(row['review_text'])\n",
    "    \n",
    "#     # Create Document instances for each chunk\n",
    "#     for chunk in chunks:\n",
    "#         document = Document(\n",
    "#             page_content=chunk,\n",
    "#             metadata={\n",
    "#                 \"review_rating\": row['review_rating'],\n",
    "#                 \"review_likes\": row['review_likes'],\n",
    "#                 \"author_app_version\": row['author_app_version']\n",
    "#             }\n",
    "#         )\n",
    "#         all_documents.append(document)\n",
    "\n",
    "# # Create FAISS index from all accumulated documents\n",
    "# print(\"Creating FAISS index...\")\n",
    "# vectordb = FAISS.from_documents(documents=all_documents, embedding=instructor_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Vector Database from the CSV File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up for the Faiss Vector Database\n",
    "embeddings = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "file_path = \"cleaned_spotify_reviews.csv\"  \n",
    "vectordb_file_path = \"faiss_index\"  # Save path for the FAISS vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ingest data into vector database\n",
    "def ingest_to_vector_db(df, text_column, embedding_model):\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=row[text_column],\n",
    "            metadata={\n",
    "                \"review_rating\": float(row[\"review_rating\"]),\n",
    "                \"review_likes\": row[\"review_likes\"],\n",
    "                \"author_app_version\": row[\"author_app_version\"]\n",
    "            }\n",
    "        )\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Use the RecursiveCharacterTextSplitter to split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    document_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Create FAISS index from documents\n",
    "    vectorstore_db = FAISS.from_documents(\n",
    "        document_chunks,\n",
    "        embedding_model,\n",
    "        distance_strategy=DistanceStrategy.COSINE,\n",
    "    )\n",
    "    \n",
    "    return vectorstore_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and process CSV\n",
    "def load_and_process_csv(csv_path, text_column, embedding_model):\n",
    "    df = pd.read_csv(csv_path, encoding='MacRoman')\n",
    "\n",
    "    # Remove rows with NaN values in critical columns\n",
    "    df = df[['review_text', 'review_rating', 'review_likes', 'author_app_version', 'review_timestamp']].dropna()\n",
    "\n",
    "    # Ingest the cleaned data into the vector database\n",
    "    vectordb = ingest_to_vector_db(df, text_column=text_column, embedding_model=embedding_model)\n",
    "    \n",
    "    # Save the vector database locally\n",
    "    vectordb.save_local(vectordb_file_path)\n",
    "    \n",
    "    print(f\"Vector database saved at '{vectordb_file_path}'\")\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colaz\\AppData\\Local\\Temp\\ipykernel_9092\\870974509.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=embeddings)\n",
      "C:\\Users\\colaz\\Projects\\ML\\LangChain\\Mekari\\Menv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\colaz\\Projects\\ML\\LangChain\\Mekari\\Menv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: name 'load_and_process_csv' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Load and process CSV data\n",
    "load_and_process_csv(file_path, text_column='review_text', embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Retriever\n",
    "vectordb = FAISS.load_local(vectordb_file_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up the Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colaz\\AppData\\Local\\Temp\\ipykernel_9092\\1758680811.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=5)  # Keep the last 5 interactions\n"
     ]
    }
   ],
   "source": [
    "# Initialize the memory for conversation\n",
    "memory = ConversationBufferWindowMemory(k=5)  # Keep the last 5 interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant designed to answer questions based on user reviews of a music streaming application.\n",
    "\n",
    "**Layer 1: Contextual Understanding**\n",
    "Please read the following user reviews carefully and provide precise answers to the questions based on the source document context (Spotify User Reviews Database) provided. Do not mention other companies.\n",
    "\n",
    "**Layer 2: User Instruction**\n",
    "Respond in a friendly and professional tone. If the answer involves comparisons to other music streaming platforms, provide specific examples from the source document context. \n",
    "\n",
    "**Layer 3: Response Guidelines**\n",
    "If the answer is not explicitly found in the source document context, kindly state: \"I'm sorry, I don't have that information.\" Please do not fabricate any answers or discuss unrelated topics.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Chain\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    input_key=\"query\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    output_key=\"result\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(review_text):\n",
    "    analysis = TextBlob(review_text)\n",
    "    sentiment_score = analysis.sentiment.polarity  # Returns a value between -1 (negative) and 1 (positive)\n",
    "\n",
    "    # Create a more informative response based on the sentiment score\n",
    "    if sentiment_score > 0.2:\n",
    "        sentiment_description = \"positive\"\n",
    "    elif sentiment_score < -0.2:\n",
    "        sentiment_description = \"negative\"\n",
    "    else:\n",
    "        sentiment_description = \"neutral\"\n",
    "\n",
    "    return f\"Based on the review, the sentiment is {sentiment_description} with a score of {sentiment_score:.2f}.\"\n",
    "\n",
    "# Function to summarize reviews\n",
    "def summarize_reviews():\n",
    "    summary_query = \"Can you summarize the key points from the reviews?\"\n",
    "    summary_response = chain({\"query\": summary_query})\n",
    "    return summary_response['result']\n",
    "\n",
    "# Function to compare features\n",
    "def compare_features(feature_1, feature_2):\n",
    "    comparison_query = f\"What do users say about {feature_1} compared to {feature_2}?\"\n",
    "    comparison_response = chain({\"query\": comparison_query})\n",
    "    return comparison_response['result']\n",
    "\n",
    "# Function to analyze trends\n",
    "def analyze_trends():\n",
    "    # Retrieve relevant documents from the vector database\n",
    "    trends_query = \"What trends are users discussing in their reviews?\"\n",
    "    # Process the query through the chain\n",
    "    response = chain({\"query\": trends_query})\n",
    "\n",
    "    # Save the context in memory after getting the response\n",
    "    memory.save_context({\"input\": trends_query}, {\"output\": response['result']})  # Store the result using the output_key specified\n",
    "    return response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity score for quality scoring\n",
    "def calculate_similarity_score(response_text, expected_text, embedding_model):\n",
    "    response_embedding = embedding_model.embed_query(response_text)\n",
    "    expected_embedding = embedding_model.embed_query(expected_text)\n",
    "    similarity_score = cosine_similarity([response_embedding], [expected_embedding])[0][0]\n",
    "    return similarity_score\n",
    "\n",
    "# Function to score the response quality\n",
    "def quality_scoring(response, expected_answer, embedding_model):\n",
    "    relevance_score = calculate_similarity_score(response, expected_answer, embedding_model)\n",
    "    accuracy_score = 1 if \"factually correct\" in response else 0.8\n",
    "    clarity_score = 1 if len(response) > 50 else 0.7\n",
    "    final_score = (0.5 * relevance_score) + (0.3 * accuracy_score) + (0.2 * clarity_score)\n",
    "    return {\n",
    "        \"relevance_score\": relevance_score,\n",
    "        \"accuracy_score\": accuracy_score,\n",
    "        \"clarity_score\": clarity_score,\n",
    "        \"final_score\": final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_query(query):\n",
    "    # Check if the query requests a summary, sentiment analysis, or feature comparison\n",
    "    if \"summarize\" in query.lower():\n",
    "        return summarize_reviews()\n",
    "    elif \"analyze sentiment for:\" in query.lower():\n",
    "        review_text = query.split(\"analyze sentiment for:\")[-1].strip()\n",
    "        return analyze_sentiment(review_text)\n",
    "    elif \"trends\" in query.lower() or \"patterns\" in query.lower():\n",
    "        return analyze_trends()\n",
    "    else:\n",
    "        # Process the query through the chain\n",
    "        response = chain({\"query\": query})\n",
    "\n",
    "        # Save the context in memory after getting the response\n",
    "        memory.save_context({\"input\": query}, {\"output\": response['result']})  # Store the result using the output_key specified\n",
    "        \n",
    "        # If no result is found in the context, return a default message\n",
    "        if \"I'm sorry, I don't have that information.\" in response['result']:\n",
    "            return \"I'm sorry, I don't have information about that topic. Please ask about user reviews or features related to the topic.\"\n",
    "        \n",
    "        return response['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colaz\\AppData\\Local\\Temp\\ipykernel_9092\\461798614.py:12: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE: Based on the user reviews, the personalized experience and the variety of features are the most appreciated aspects of our application. Users love the ability to customize their music experience and the wide range of features available to them. Some examples of these features include curated playlists, personalized recommendations, and the ability to explore new music.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"What are the specific features or aspects that users appreciate the most in our application?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE: From the reviews, it seems that users appreciate the ability to read specific parts of reviews and that all reviews are read. They also value the overall review system.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"Can you summarize the reviews?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE: Based on user reviews, it seems that users are most likely to compare our application with other music streaming platforms such as Apple Music or Amazon Music. However, our user interface has been praised as the best among all music streaming apps.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"In comparison to our application, which music streaming platform are users most likely to compare ours with?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE: Based on the user reviews, the primary reasons for dissatisfaction with Spotify seem to be the monetization of basic features and a lack of clear value in using the platform.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"What are the primary reasons users express dissatisfaction with Spotify?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The users are discussing changes and improvements being made to the reviews, as well as the frequency of advertisements on the platform.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"Can you identify emerging trends or patterns in recent user reviews that may impact our product strategy?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Hallucinations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have information about that topic. Please ask about user reviews or features related to the topic.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"What are the primary reasons users express dissatisfaction with Google?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have information about that topic. Please ask about user reviews or features related to the topic.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"Who won the world cup in 2034?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have information about that topic. Please ask about user reviews or features related to the topic.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"What color is your house?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have information about that topic. Please ask about user reviews or features related to the topic.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"What are the primary reasons users express dissatisfaction with Counter Strike?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have information about that topic. Please ask about user reviews or features related to the topic.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"What is the best features in Indofood?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have information about that topic. Please ask about user reviews or features related to the topic.\n"
     ]
    }
   ],
   "source": [
    "response = handle_query(\"What is Nvidia current stock price?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '90890hubhjbhj',\n",
       " 'result': \"\\nI'm sorry, I don't have that information. Could you please provide a question related to the user reviews for a music streaming application?\",\n",
       " 'source_documents': [Document(metadata={'review_rating': 5.0, 'review_likes': 0, 'author_app_version': '8.4.3.479'}, page_content='90200000'),\n",
       "  Document(metadata={'review_rating': 5.0, 'review_likes': 0, 'author_app_version': '8.6.46.886'}, page_content='08022021'),\n",
       "  Document(metadata={'review_rating': 1.0, 'review_likes': 0, 'author_app_version': '8.7.92.521'}, page_content='692023'),\n",
       "  Document(metadata={'review_rating': 3.0, 'review_likes': 0, 'author_app_version': '8.6.44.851'}, page_content='118807990911_')]}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"90890hubhjbhj\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Menv",
   "language": "python",
   "name": "menv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
